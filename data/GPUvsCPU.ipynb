{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GPUvsCPU.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "-TDfPfHkcSxr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zl1oeewTbWIV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1071
        },
        "outputId": "9e5eaf88-74b5-446c-cfe7-55391eae341c"
      },
      "cell_type": "code",
      "source": [
        "import time\n",
        "start_time = time.time()\n",
        "import torch \n",
        "import torch.nn as nn \n",
        "import torchvision.datasets as dsets \n",
        "import torchvision.transforms as transforms \n",
        "import torch.nn.functional as fun\n",
        "from torch.autograd import Variable \n",
        "device = torch.device(\"cuda:0\")\n",
        "# Hyper Parameters  \n",
        "input_size = 784\n",
        "num_classes = 10\n",
        "hidden_size = 300  \n",
        "num_epochs = 10\n",
        "batch_size = 100\n",
        "learning_rate = 0.2\n",
        "# MNIST Dataset (Images and Labels) \n",
        "train_dataset = dsets.MNIST(root ='./data', \n",
        "\t\t\t\t\t\t\ttrain = True, \n",
        "\t\t\t\t\t\t\ttransform = transforms.ToTensor(), \n",
        "\t\t\t\t\t\t\tdownload = True) \n",
        "\n",
        "test_dataset = dsets.MNIST(root ='./data', \n",
        "\t\t\t\t\t\ttrain = False, \n",
        "\t\t\t\t\t\ttransform = transforms.ToTensor()) \n",
        "\n",
        "# Dataset Loader (Input Pipline) \n",
        "train_loader = torch.utils.data.DataLoader(dataset = train_dataset, \n",
        "\t\t\t\t\t\t\t\t\t\tbatch_size = batch_size, \n",
        "\t\t\t\t\t\t\t\t\t\tshuffle = True) \n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset = test_dataset, \n",
        "\t\t\t\t\t\t\t\t\t\tbatch_size = batch_size, \n",
        "\t\t\t\t\t\t\t\t\t\tshuffle = False) \n",
        "\n",
        "# Model\n",
        "class LeNET(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNET, self).__init__() \n",
        "        self.conv1 = nn.Conv2d(1, 6, 7, 1, 0)# 1 канал(черно-белый),если картинка цветная то 3 - так как RGB; 6 - выходов или подслоев,\n",
        "        #каждый подслой имеет свои веса ;#input,output,kernel,stride,padding; выход тензоры 22x22 (28-kernel+1)\n",
        "        self.maxpool1 = nn.MaxPool2d(2) #выход тензоры 11x11 по 6 каналам\n",
        "        self.conv2 = nn.Conv2d(6, 16, 2, 1, 1)#padding=1 =>13x13    (13-2+1)=> выход 12x12 по 16 каналам\n",
        "        self.maxpool2 = nn.MaxPool2d(2)#выход 6x6 по 16 каналам => 6x6x16=576\n",
        "        self.relu = nn.ReLU() \n",
        "        self.dropout2d = nn.Dropout2d()\n",
        "        self.fc1 = nn.Linear(576, 1000)\n",
        "        self.fc2 = nn.Linear(1000, 10)\n",
        "\n",
        "    def forward(self, x): \n",
        "        out = self.relu(self.maxpool1(self.conv1(x)))\n",
        "        out = self.relu(self.maxpool2(self.dropout2d(self.conv2(out))))\n",
        "        out = out.view(-1, 576)\n",
        "        out = self.relu(self.fc1(out)) \n",
        "        out = self.fc2(out) \n",
        "        return out \n",
        "\n",
        "model = LeNET() \n",
        "model.to(device)\n",
        "# Loss and Optimizer \n",
        "# Softmax is internally computed.  \n",
        "criterion = nn.CrossEntropyLoss() \n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate) \n",
        "  \n",
        "# Training the Model\n",
        "\n",
        "for epoch in range(num_epochs): \n",
        "    for i, (images, labels) in enumerate(train_loader): \n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        # Forward + Backward + Optimize \n",
        "        optimizer.zero_grad() \n",
        "        outputs=outputs.to(device)\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels) \n",
        "        loss.backward() \n",
        "        optimizer.step() \n",
        "  \n",
        "        if (i + 1) % 100 == 0: \n",
        "            print('Epoch: [% d/% d], Step: [% d/% d], Loss: %.4f'\n",
        "                  % (epoch + 1, num_epochs, i + 1, \n",
        "                     len(train_dataset) // batch_size, loss.item()))     \n",
        "#Test the Model \n",
        "correct = 0\n",
        "total = 0\n",
        "for images, labels in test_loader:\n",
        "    images = images.to(device)\n",
        "    labels = labels.to(device)\n",
        "    outputs = model(images) \n",
        "    _, predicted = torch.max(outputs.data, 1) \n",
        "    total += labels.size(0) \n",
        "    correct += (predicted == labels).sum() \n",
        "  \n",
        "print('Accuracy of the model on the 10000 test images: % d %%' % ( \n",
        "            100 * correct / total))\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: [ 1/ 10], Step: [ 100/ 600], Loss: 0.5213\n",
            "Epoch: [ 1/ 10], Step: [ 200/ 600], Loss: 0.2601\n",
            "Epoch: [ 1/ 10], Step: [ 300/ 600], Loss: 0.3771\n",
            "Epoch: [ 1/ 10], Step: [ 400/ 600], Loss: 0.2562\n",
            "Epoch: [ 1/ 10], Step: [ 500/ 600], Loss: 0.2240\n",
            "Epoch: [ 1/ 10], Step: [ 600/ 600], Loss: 0.1363\n",
            "Epoch: [ 2/ 10], Step: [ 100/ 600], Loss: 0.1723\n",
            "Epoch: [ 2/ 10], Step: [ 200/ 600], Loss: 0.1307\n",
            "Epoch: [ 2/ 10], Step: [ 300/ 600], Loss: 0.0578\n",
            "Epoch: [ 2/ 10], Step: [ 400/ 600], Loss: 0.1412\n",
            "Epoch: [ 2/ 10], Step: [ 500/ 600], Loss: 0.1151\n",
            "Epoch: [ 2/ 10], Step: [ 600/ 600], Loss: 0.0739\n",
            "Epoch: [ 3/ 10], Step: [ 100/ 600], Loss: 0.1553\n",
            "Epoch: [ 3/ 10], Step: [ 200/ 600], Loss: 0.1389\n",
            "Epoch: [ 3/ 10], Step: [ 300/ 600], Loss: 0.0989\n",
            "Epoch: [ 3/ 10], Step: [ 400/ 600], Loss: 0.1054\n",
            "Epoch: [ 3/ 10], Step: [ 500/ 600], Loss: 0.1075\n",
            "Epoch: [ 3/ 10], Step: [ 600/ 600], Loss: 0.1978\n",
            "Epoch: [ 4/ 10], Step: [ 100/ 600], Loss: 0.0199\n",
            "Epoch: [ 4/ 10], Step: [ 200/ 600], Loss: 0.0424\n",
            "Epoch: [ 4/ 10], Step: [ 300/ 600], Loss: 0.1411\n",
            "Epoch: [ 4/ 10], Step: [ 400/ 600], Loss: 0.1579\n",
            "Epoch: [ 4/ 10], Step: [ 500/ 600], Loss: 0.1948\n",
            "Epoch: [ 4/ 10], Step: [ 600/ 600], Loss: 0.1059\n",
            "Epoch: [ 5/ 10], Step: [ 100/ 600], Loss: 0.1090\n",
            "Epoch: [ 5/ 10], Step: [ 200/ 600], Loss: 0.1402\n",
            "Epoch: [ 5/ 10], Step: [ 300/ 600], Loss: 0.0421\n",
            "Epoch: [ 5/ 10], Step: [ 400/ 600], Loss: 0.0353\n",
            "Epoch: [ 5/ 10], Step: [ 500/ 600], Loss: 0.0285\n",
            "Epoch: [ 5/ 10], Step: [ 600/ 600], Loss: 0.0899\n",
            "Epoch: [ 6/ 10], Step: [ 100/ 600], Loss: 0.0360\n",
            "Epoch: [ 6/ 10], Step: [ 200/ 600], Loss: 0.0980\n",
            "Epoch: [ 6/ 10], Step: [ 300/ 600], Loss: 0.0543\n",
            "Epoch: [ 6/ 10], Step: [ 400/ 600], Loss: 0.0522\n",
            "Epoch: [ 6/ 10], Step: [ 500/ 600], Loss: 0.0911\n",
            "Epoch: [ 6/ 10], Step: [ 600/ 600], Loss: 0.0570\n",
            "Epoch: [ 7/ 10], Step: [ 100/ 600], Loss: 0.0216\n",
            "Epoch: [ 7/ 10], Step: [ 200/ 600], Loss: 0.0649\n",
            "Epoch: [ 7/ 10], Step: [ 300/ 600], Loss: 0.0468\n",
            "Epoch: [ 7/ 10], Step: [ 400/ 600], Loss: 0.0450\n",
            "Epoch: [ 7/ 10], Step: [ 500/ 600], Loss: 0.0350\n",
            "Epoch: [ 7/ 10], Step: [ 600/ 600], Loss: 0.0374\n",
            "Epoch: [ 8/ 10], Step: [ 100/ 600], Loss: 0.0359\n",
            "Epoch: [ 8/ 10], Step: [ 200/ 600], Loss: 0.1128\n",
            "Epoch: [ 8/ 10], Step: [ 300/ 600], Loss: 0.0223\n",
            "Epoch: [ 8/ 10], Step: [ 400/ 600], Loss: 0.1284\n",
            "Epoch: [ 8/ 10], Step: [ 500/ 600], Loss: 0.1204\n",
            "Epoch: [ 8/ 10], Step: [ 600/ 600], Loss: 0.0394\n",
            "Epoch: [ 9/ 10], Step: [ 100/ 600], Loss: 0.0138\n",
            "Epoch: [ 9/ 10], Step: [ 200/ 600], Loss: 0.0632\n",
            "Epoch: [ 9/ 10], Step: [ 300/ 600], Loss: 0.1165\n",
            "Epoch: [ 9/ 10], Step: [ 400/ 600], Loss: 0.0589\n",
            "Epoch: [ 9/ 10], Step: [ 500/ 600], Loss: 0.0714\n",
            "Epoch: [ 9/ 10], Step: [ 600/ 600], Loss: 0.1117\n",
            "Epoch: [ 10/ 10], Step: [ 100/ 600], Loss: 0.0030\n",
            "Epoch: [ 10/ 10], Step: [ 200/ 600], Loss: 0.0927\n",
            "Epoch: [ 10/ 10], Step: [ 300/ 600], Loss: 0.0032\n",
            "Epoch: [ 10/ 10], Step: [ 400/ 600], Loss: 0.0337\n",
            "Epoch: [ 10/ 10], Step: [ 500/ 600], Loss: 0.0544\n",
            "Epoch: [ 10/ 10], Step: [ 600/ 600], Loss: 0.0084\n",
            "Accuracy of the model on the 10000 test images:  98 %\n",
            "--- 82.7718448638916 seconds ---\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kJy3X8zEbish",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# http://pytorch.org/\n",
        "from os.path import exists\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
        "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
        "\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision\n",
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G8jN-hSphOJR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1071
        },
        "outputId": "e7710e82-0552-499c-cc20-daa12b132ea4"
      },
      "cell_type": "code",
      "source": [
        "import time\n",
        "start_time = time.time()\n",
        "import torch \n",
        "import torch.nn as nn \n",
        "import torchvision.datasets as dsets \n",
        "import torchvision.transforms as transforms \n",
        "import torch.nn.functional as fun\n",
        "from torch.autograd import Variable \n",
        "device = torch.device(\"cuda:0\")\n",
        "# Hyper Parameters  \n",
        "input_size = 784\n",
        "num_classes = 10\n",
        "hidden_size = 300  \n",
        "num_epochs = 10\n",
        "batch_size = 100\n",
        "learning_rate = 0.2\n",
        "# MNIST Dataset (Images and Labels) \n",
        "train_dataset = dsets.MNIST(root ='./data', \n",
        "\t\t\t\t\t\t\ttrain = True, \n",
        "\t\t\t\t\t\t\ttransform = transforms.ToTensor(), \n",
        "\t\t\t\t\t\t\tdownload = True) \n",
        "\n",
        "test_dataset = dsets.MNIST(root ='./data', \n",
        "\t\t\t\t\t\ttrain = False, \n",
        "\t\t\t\t\t\ttransform = transforms.ToTensor()) \n",
        "\n",
        "# Dataset Loader (Input Pipline) \n",
        "train_loader = torch.utils.data.DataLoader(dataset = train_dataset, \n",
        "\t\t\t\t\t\t\t\t\t\tbatch_size = batch_size, \n",
        "\t\t\t\t\t\t\t\t\t\tshuffle = True) \n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset = test_dataset, \n",
        "\t\t\t\t\t\t\t\t\t\tbatch_size = batch_size, \n",
        "\t\t\t\t\t\t\t\t\t\tshuffle = False) \n",
        "\n",
        "# Model\n",
        "class LeNET(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNET, self).__init__() \n",
        "        self.conv1 = nn.Conv2d(1, 6, 7, 1, 0)# 1 канал(черно-белый),если картинка цветная то 3 - так как RGB; 6 - выходов или подслоев,\n",
        "        #каждый подслой имеет свои веса ;#input,output,kernel,stride,padding; выход тензоры 22x22 (28-kernel+1)\n",
        "        self.maxpool1 = nn.MaxPool2d(2) #выход тензоры 11x11 по 6 каналам\n",
        "        self.conv2 = nn.Conv2d(6, 16, 2, 1, 1)#padding=1 =>13x13    (13-2+1)=> выход 12x12 по 16 каналам\n",
        "        self.maxpool2 = nn.MaxPool2d(2)#выход 6x6 по 16 каналам => 6x6x16=576\n",
        "        self.relu = nn.ReLU() \n",
        "        self.dropout2d = nn.Dropout2d()\n",
        "        self.fc1 = nn.Linear(576, 1000)\n",
        "        self.fc2 = nn.Linear(1000, 10)\n",
        "\n",
        "    def forward(self, x): \n",
        "        out = self.relu(self.maxpool1(self.conv1(x)))\n",
        "        out = self.relu(self.maxpool2(self.dropout2d(self.conv2(out))))\n",
        "        out = out.view(-1, 576)\n",
        "        out = self.relu(self.fc1(out)) \n",
        "        out = self.fc2(out) \n",
        "        return out \n",
        "\n",
        "model = LeNET() \n",
        "#model.to(device)\n",
        "# Loss and Optimizer \n",
        "# Softmax is internally computed.  \n",
        "criterion = nn.CrossEntropyLoss() \n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate) \n",
        "  \n",
        "# Training the Model\n",
        "\n",
        "for epoch in range(num_epochs): \n",
        "    for i, (images, labels) in enumerate(train_loader): \n",
        "        #images = images.to(device)\n",
        "        #labels = labels.to(device)\n",
        "        # Forward + Backward + Optimize \n",
        "        optimizer.zero_grad() \n",
        "        outputs=outputs.to(device)\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels) \n",
        "        loss.backward() \n",
        "        optimizer.step() \n",
        "  \n",
        "        if (i + 1) % 100 == 0: \n",
        "            print('Epoch: [% d/% d], Step: [% d/% d], Loss: %.4f'\n",
        "                  % (epoch + 1, num_epochs, i + 1, \n",
        "                     len(train_dataset) // batch_size, loss.item()))     \n",
        "#Test the Model \n",
        "correct = 0\n",
        "total = 0\n",
        "for images, labels in test_loader:\n",
        "    #images = images.to(device)\n",
        "    #labels = labels.to(device)\n",
        "    outputs = model(images) \n",
        "    _, predicted = torch.max(outputs.data, 1) \n",
        "    total += labels.size(0) \n",
        "    correct += (predicted == labels).sum() \n",
        "  \n",
        "print('Accuracy of the model on the 10000 test images: % d %%' % ( \n",
        "            100 * correct / total))\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: [ 1/ 10], Step: [ 100/ 600], Loss: 0.3039\n",
            "Epoch: [ 1/ 10], Step: [ 200/ 600], Loss: 0.2818\n",
            "Epoch: [ 1/ 10], Step: [ 300/ 600], Loss: 0.2916\n",
            "Epoch: [ 1/ 10], Step: [ 400/ 600], Loss: 0.0871\n",
            "Epoch: [ 1/ 10], Step: [ 500/ 600], Loss: 0.1278\n",
            "Epoch: [ 1/ 10], Step: [ 600/ 600], Loss: 0.1357\n",
            "Epoch: [ 2/ 10], Step: [ 100/ 600], Loss: 0.1457\n",
            "Epoch: [ 2/ 10], Step: [ 200/ 600], Loss: 0.0587\n",
            "Epoch: [ 2/ 10], Step: [ 300/ 600], Loss: 0.1125\n",
            "Epoch: [ 2/ 10], Step: [ 400/ 600], Loss: 0.0925\n",
            "Epoch: [ 2/ 10], Step: [ 500/ 600], Loss: 0.0613\n",
            "Epoch: [ 2/ 10], Step: [ 600/ 600], Loss: 0.1161\n",
            "Epoch: [ 3/ 10], Step: [ 100/ 600], Loss: 0.1067\n",
            "Epoch: [ 3/ 10], Step: [ 200/ 600], Loss: 0.1692\n",
            "Epoch: [ 3/ 10], Step: [ 300/ 600], Loss: 0.0549\n",
            "Epoch: [ 3/ 10], Step: [ 400/ 600], Loss: 0.0767\n",
            "Epoch: [ 3/ 10], Step: [ 500/ 600], Loss: 0.1389\n",
            "Epoch: [ 3/ 10], Step: [ 600/ 600], Loss: 0.0757\n",
            "Epoch: [ 4/ 10], Step: [ 100/ 600], Loss: 0.1258\n",
            "Epoch: [ 4/ 10], Step: [ 200/ 600], Loss: 0.0258\n",
            "Epoch: [ 4/ 10], Step: [ 300/ 600], Loss: 0.1722\n",
            "Epoch: [ 4/ 10], Step: [ 400/ 600], Loss: 0.0819\n",
            "Epoch: [ 4/ 10], Step: [ 500/ 600], Loss: 0.0350\n",
            "Epoch: [ 4/ 10], Step: [ 600/ 600], Loss: 0.0802\n",
            "Epoch: [ 5/ 10], Step: [ 100/ 600], Loss: 0.1228\n",
            "Epoch: [ 5/ 10], Step: [ 200/ 600], Loss: 0.0531\n",
            "Epoch: [ 5/ 10], Step: [ 300/ 600], Loss: 0.0590\n",
            "Epoch: [ 5/ 10], Step: [ 400/ 600], Loss: 0.0716\n",
            "Epoch: [ 5/ 10], Step: [ 500/ 600], Loss: 0.1158\n",
            "Epoch: [ 5/ 10], Step: [ 600/ 600], Loss: 0.0570\n",
            "Epoch: [ 6/ 10], Step: [ 100/ 600], Loss: 0.0536\n",
            "Epoch: [ 6/ 10], Step: [ 200/ 600], Loss: 0.0250\n",
            "Epoch: [ 6/ 10], Step: [ 300/ 600], Loss: 0.0234\n",
            "Epoch: [ 6/ 10], Step: [ 400/ 600], Loss: 0.0157\n",
            "Epoch: [ 6/ 10], Step: [ 500/ 600], Loss: 0.0213\n",
            "Epoch: [ 6/ 10], Step: [ 600/ 600], Loss: 0.0276\n",
            "Epoch: [ 7/ 10], Step: [ 100/ 600], Loss: 0.0205\n",
            "Epoch: [ 7/ 10], Step: [ 200/ 600], Loss: 0.0325\n",
            "Epoch: [ 7/ 10], Step: [ 300/ 600], Loss: 0.0347\n",
            "Epoch: [ 7/ 10], Step: [ 400/ 600], Loss: 0.1668\n",
            "Epoch: [ 7/ 10], Step: [ 500/ 600], Loss: 0.0560\n",
            "Epoch: [ 7/ 10], Step: [ 600/ 600], Loss: 0.0635\n",
            "Epoch: [ 8/ 10], Step: [ 100/ 600], Loss: 0.0364\n",
            "Epoch: [ 8/ 10], Step: [ 200/ 600], Loss: 0.0067\n",
            "Epoch: [ 8/ 10], Step: [ 300/ 600], Loss: 0.0129\n",
            "Epoch: [ 8/ 10], Step: [ 400/ 600], Loss: 0.0457\n",
            "Epoch: [ 8/ 10], Step: [ 500/ 600], Loss: 0.0304\n",
            "Epoch: [ 8/ 10], Step: [ 600/ 600], Loss: 0.0500\n",
            "Epoch: [ 9/ 10], Step: [ 100/ 600], Loss: 0.0107\n",
            "Epoch: [ 9/ 10], Step: [ 200/ 600], Loss: 0.0200\n",
            "Epoch: [ 9/ 10], Step: [ 300/ 600], Loss: 0.0877\n",
            "Epoch: [ 9/ 10], Step: [ 400/ 600], Loss: 0.0347\n",
            "Epoch: [ 9/ 10], Step: [ 500/ 600], Loss: 0.0233\n",
            "Epoch: [ 9/ 10], Step: [ 600/ 600], Loss: 0.0422\n",
            "Epoch: [ 10/ 10], Step: [ 100/ 600], Loss: 0.0155\n",
            "Epoch: [ 10/ 10], Step: [ 200/ 600], Loss: 0.0798\n",
            "Epoch: [ 10/ 10], Step: [ 300/ 600], Loss: 0.0034\n",
            "Epoch: [ 10/ 10], Step: [ 400/ 600], Loss: 0.0275\n",
            "Epoch: [ 10/ 10], Step: [ 500/ 600], Loss: 0.0458\n",
            "Epoch: [ 10/ 10], Step: [ 600/ 600], Loss: 0.0907\n",
            "Accuracy of the model on the 10000 test images:  98 %\n",
            "--- 234.91320705413818 seconds ---\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}